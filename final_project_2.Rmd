---
title: "p8130_final_project_2"
output: pdf_document
date: "2024-12-19"
---

# Project 2: Breast cancer survival prediction

```{r, include=FALSE}
library(tidyverse)
library(knitr)
library(car) 
library(e1071)
library(glmnet)
```

## Data exploration

### Descriptive table with summary statistics

```{r}
data <- read.csv("Project_2_data.csv")
head(data,10)

numerical_summary <- data %>%
  select_if(is.numeric) %>%
  summarise_all(list(
    count = ~sum(!is.na(.)),
    mean = mean,
    std = sd,
    min = min,
    median = median,
    max = max
  )) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Variable", "Statistic"), sep = "_")

formatted_summary <- numerical_summary %>%
  pivot_wider(names_from = Statistic, values_from = Value)

kable(formatted_summary, col.names = c("Variable", "Count", "Mean", "Std", "Min", "Median", "Max"), caption = "Numerical Variables Summary Statistics")
```

```{r}
categorical_vars <- data %>% select_if(is.character)

category_summary <- categorical_vars %>%
  gather(Variable, Category) %>%
  group_by(Variable, Category) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = round((Count / sum(Count)) * 100, 2)) %>%
  arrange(Variable, desc(Count))

formatted_summary <- category_summary %>%
  group_by(Variable) %>%
  mutate(Variable = ifelse(row_number() == 1, Variable, ""))

kable(formatted_summary, col.names = c("Variable", "Category", "Count", "Percentage (%)"), caption = "Category Distribution of Categorical Variables")
```

### Explore the Distribution of the Outcome (Status: Dead / Alive)
```{r}
status_distribution <- data %>%
  group_by(Status) %>%
  summarise(Count = n()) %>%
  mutate(Proportion = Count / sum(Count))

kable(status_distribution, col.names = c("Status", "Count", "Proportion"), caption = "Distribution of Survival Status (Dead/Alive)")

ggplot(data, aes(x = Status, fill = Status)) +
  geom_bar() +
  labs(title = "Distribution of Survival Status", x = "Status", y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("lightblue", "pink"))
```

For logistic regression, the binary outcome variable (Status: Dead/Alive) does not require transformation, as logistic regression inherently models binary outcomes.

### Transformation
```{r}
# Identify numerical variables
numerical_vars <- data %>%
  select_if(is.numeric) %>%
  select(-`Survival.Months`)

# Display the list of numerical variables
names(numerical_vars)
# Convert Status to a binary numeric variable
data$Status <- ifelse(data$Status == "Dead", 1, 0)

# Scatterplots for each numerical variable against the logit
logit <- function(p) log(p / (1 - p))  # Logit function

numerical_vars %>%
  names() %>%
  map(~ ggplot(data, aes(x = .data[[.x]], y = Status)) +
        stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
        geom_point(alpha = 0.5) +
        labs(title = paste("Relationship Between", .x, "and Logit of Status"), 
             x = .x, 
             y = "Logit(Status)") +
        theme_minimal())

# Calculate skewness for numerical variables
numerical_skewness <- numerical_vars %>%
  map_df(~ tibble(Variable = deparse(substitute(.)),
                  Skewness = skewness(., na.rm = TRUE)))

# Correct the Variable column
numerical_skewness <- tibble(
  Variable = colnames(numerical_vars),
  Skewness = sapply(numerical_vars, skewness, na.rm = TRUE)
)

# Display the skewness table
kable(numerical_skewness, col.names = c("Variable", "Skewness"), caption = "Skewness of Numerical Variables")
```

After our initial detection, we found out that: 

* Reginol.Node.Positive variable show slightly nonlinear with the logit of Status, it need transformation.

* The skewness analysis reveals that Age (-0.22) has a roughly symmetric distribution, requiring no transformation. Tumor Size (1.74) shows moderate right skewness, suggesting a potential log transformation to normalize the distribution, though it may not be strictly necessary. Regional Node Examined (0.83) has mild positive skewness and can likely be retained in its current form unless further diagnostics indicate otherwise. Reginol Node Positive (2.70), with significant right skewness, would benefit from a log transformation to reduce skewness and stabilize its relationship with the logit in the logistic regression model. These adjustments ensure numerical variables are well-prepared for regression analysis.

Base on the analysis above, try to make log transformation on Reginol Node Positive & Tumor Size.

```{r}
data <- data %>%
  mutate(
    Log_Reginol_Node_Positive = log1p(`Reginol.Node.Positive`),
    Log_Tumor_Size = log1p(`Tumor.Size`)
  )
transformed_skewness <- data %>%
  select(Log_Reginol_Node_Positive, Log_Tumor_Size) %>%
  summarise_all(~ skewness(.))

# Combine with variable names
transformed_skewness_table <- tibble(
  Variable = c("Log_Reginol_Node_Positive", "Log_Tumor_Size"),
  Skewness = as.numeric(transformed_skewness)
)

# Display the updated skewness table
kable(transformed_skewness_table, col.names = c("Variable", "Skewness"), caption = "Skewness of Transformed Variables")

## Plots for Transformed Variables Against Logit of Status
logit <- function(p) log(p / (1 - p))  # Logit function

# Plot for Log_Reginol_Node_Positive
ggplot(data, aes(x = Log_Reginol_Node_Positive, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Reginol_Node_Positive and Logit of Status", x = "Log_Reginol_Node_Positive", y = "Logit(Status)") +
  theme_minimal()

# Plot for Log_Tumor_Size
ggplot(data, aes(x = Log_Tumor_Size, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Tumor_Size and Logit of Status", x = "Log_Tumor_Size", y = "Logit(Status)") +
  theme_minimal()
```

Comments:

* For Log_Reginol_Node_Positive, the skewness improved from 2.70 to 0.99, indicating a significant reduction in skewness. While still slightly positively skewed, the value is now within an acceptable range for modeling.

* For Log_Tumor_Size, the skewness reduced from 1.74 to -0.09, making it almost symmetric. This transformation effectively normalized the variable.

* For Log_Reginol_Node_Positive, the log transformation on Reginol.Node.Positive likely improved its relationship with the logit as well. 

* For Log_Tumor_Size, after the transformation the linearity with the logit has not significantly improved, but at least the skewness reduced.

As a result, we should definitely conduct a log transformation on Reginol.Node.Positive and Tumor.Size.

```{r}
data=data |>
  select(-`Reginol.Node.Positive`, -`Tumor.Size`)
```

```{r}
# Compute correlation coefficients among numeric variables
selected_vars <- c("Age", "Log_Reginol_Node_Positive", "Log_Tumor_Size","Regional.Node.Examined")

subset_data <- data[, selected_vars]

if (all(sapply(subset_data, is.numeric))) {
  correlation_matrix <- cor(subset_data, use = "pairwise.complete.obs")
  print(correlation_matrix)
}
```

None of the correlation coefficients between numeric variables exceed 0.5, indicating that there is no strong linear relationship between each pair of numeric variables.

```{r}
# Identify highly consistent category variables
contingency_table <- table(data[["differentiate"]], data[["Grade"]])
print(contingency_table)
```

We discover that complete linear dependency exist among Grade and differentiate, so we can only include one of them in the prediction model, so differentiate is excluded.

```{r}
data=data |>
  select(-differentiate)
```

Finally, we need to change all the catagorical variables to dummy variables:
```{r}
categorical_vars <- data %>%
  select_if(is.character) %>%
  names()

data_final <- data %>%
  mutate(across(all_of(categorical_vars), ~ as.factor(.))) %>%  
  model.matrix(~ . - 1, data = .) %>%  
  as.data.frame() 

data_final = data_final |>
  select(-`Survival.Months`)

head(data_final,10)
```

After basic data preprocessing, we use lasso regression to help select the variables used as predictors.

```{r}
x <- model.matrix(Status ~ ., data = data_final)[, -1]
y <- data_final$Status

# Perform cross-validation for Lasso regression
lasso_cv <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# Get the optimal regularization parameter lambda
best_lambda <- lasso_cv$lambda.min
print(paste("Optimal lambda:", best_lambda))

# Fit the Lasso model using the optimal lambda
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best_lambda)

# Extract the coefficients from the Lasso model
lasso_coefficients <- coef(lasso_model)

# Convert coefficients to a standard matrix format
lasso_coefficients_matrix <- as.matrix(lasso_coefficients)

# Extract variable names with non-zero coefficients (excluding the intercept)
selected_vars <- rownames(lasso_coefficients_matrix)[lasso_coefficients_matrix[, 1] != 0][-1]

# Output the selected variables
print("Selected variables:")
print(selected_vars)

# Construct the logistic regression formula
final_formula <- as.formula(paste("Status ~", paste(selected_vars, collapse = " + ")))

# Fit the final logistic regression model
final_model <- glm(final_formula, data = data_final, family = "binomial")

# Output the summary of the final model
summary(final_model)
```

```{r}
# Check for linear dependencies (aliased variables)
alias_info <- alias(final_model)
print(alias_info)

if (!is.null(alias_info$Complete)) {
  aliased_vars <- rownames(alias_info$Complete)
  print("Aliased (linearly dependent) variables:")
  print(aliased_vars)
} else {
  print("No aliased coefficients found.")
}
```

The variable "X6th" is excluded from the model because one of its levels, X6th.StageIIIC, was identified as an aliased (linearly dependent) variable. Specifically, X6th.StageIIIC was found to be perfectly correlated with N.StageN3, which can cause multicollinearity issues and lead to instability in coefficient estimation. Removing X6th ensures a more robust and interpretable model.

```{r}
# Remove X6th-related variables from selected_vars
X6th_N3 <- c("N.StageN3", "X6th.StageIIIC")
selected_vars_updated <- setdiff(selected_vars, X6th_N3)

# Output the updated selected variables
print("Updated selected variables:")
print(selected_vars_updated)

# Construct the updated logistic regression formula
updated_formula <- as.formula(paste("Status ~", paste(selected_vars_updated, collapse = " + ")))

# Fit the updated logistic regression model
final_model_updated <- glm(updated_formula, data = data_final, family = "binomial")

# Output the summary of the updated model
summary(final_model_updated)

```

```{r}
# Calculate Variance Inflation Factor (VIF) for the updated model
vif_values_updated <- vif(final_model_updated)
print("Variance Inflation Factors (VIF) for the updated model:")
print(vif_values_updated)

vif_df <- data.frame(
  Variable = names(vif_values_updated),
  VIF = vif_values_updated
)

ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_hline(yintercept = 5, color = "red", linetype = "dashed") +
  labs(title = "Variance Inflation Factors (VIF)",
       x = "Predictor Variables",
       y = "VIF") +
  coord_flip() +
  theme_minimal()
```

```{r}

threshold <- 0.5 
# Predict probabilities using the updated model
pred_probs_updated <- predict(final_model_updated, type = "response")

# Set the same classification threshold
pred_classes_updated <- ifelse(pred_probs_updated >= threshold, 1, 0)

# Generate the confusion matrix for the updated model
conf_matrix_updated <- confusionMatrix(as.factor(pred_classes_updated), as.factor(y), positive = "1")
print(conf_matrix_updated)

```

```{r}
# Install and load necessary package
if(!require(pROC)) install.packages("pROC")
library(pROC)

# Compute ROC curve
roc_obj <- roc(y, pred_probs_updated)

# Plot ROC curve
plot(roc_obj, col = "blue", main = "ROC Curve for Final Logistic Regression Model")
abline(a = 0, b = 1, lty = 2, col = "red")  # Diagonal line for reference

# Calculate and print AUC
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 3)))

```
```{r}
# Install and load necessary package
if(!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Create calibration data by binning predicted probabilities
calibration_data <- data.frame(
  predicted = pred_probs_updated,
  actual = y
)

# Plot calibration
ggplot(calibration_data, aes(x = predicted, y = actual)) +
  stat_summary_bin(fun = mean, bins = 10, geom = "line", color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Calibration Plot", x = "Predicted Probability", y = "Observed Proportion") +
  theme_minimal()

```

```{r}
# Ensure 'Race' is included in data_final
if(!"Race" %in% colnames(data_final)){
  data_final$Race <- data$Race
}

# Define race groups: "White" and "Black_Other"
data_final <- data_final %>%
  mutate(Race_Group = ifelse(Race == "White", "White", "Black_Other"))

# Function to compute performance metrics
compute_metrics <- function(actual, predicted, probs, group_name) {
  cm <- confusionMatrix(as.factor(predicted), as.factor(actual), positive = "1")
  roc_obj <- roc(actual, probs)
  auc_val <- auc(roc_obj)
  
  return(data.frame(
    Group = group_name,
    Accuracy = round(cm$overall['Accuracy'], 3),
    Precision = round(cm$byClass['Precision'], 3),
    Recall = round(cm$byClass['Recall'], 3),
    F1 = round(cm$byClass['F1'], 3),
    AUC = round(auc_val, 3)
  ))
}

# Initialize list to store metrics
metrics_list <- list()

# Iterate over each race group
for(group in unique(data_final$Race_Group)) {
  idx <- which(data_final$Race_Group == group)
  actual_group <- y[idx]
  probs_group <- pred_probs_updated[idx]
  predicted_group <- pred_classes_updated[idx]
  
  metrics <- compute_metrics(actual_group, predicted_group, probs_group, group)
  metrics_list[[group]] <- metrics
}

# Combine metrics into a single data frame
metrics_df <- bind_rows(metrics_list)
print(metrics_df)


```
```{r}
# Compute ROC curves for each group
roc_white <- roc(y[data_final$Race_Group == "White"], pred_probs_updated[data_final$Race_Group == "White"])
roc_black_other <- roc(y[data_final$Race_Group == "Black_Other"], pred_probs_updated[data_final$Race_Group == "Black_Other"])

# Perform DeLong's test to compare AUCs
delong_test <- roc.test(roc_white, roc_black_other, method = "delong")
print(delong_test)

```


```{r}
# Calculate Cook's Distance
cooks_d <- cooks.distance(final_model_updated)

# Plot Cook's Distance
plot(cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4/(nrow(data_final) - length(final_model_updated$coefficients) - 1), col = "red", lty = 2)

# Identify influential points
influential_threshold <- 4/(nrow(data_final) - length(final_model_updated$coefficients) - 1)
influential_points <- which(cooks_d > influential_threshold)
print(paste("Number of influential points:", length(influential_points)))

# Inspect influential observations
influential_data <- data_final[influential_points, ]
kable(head(influential_data, 10), caption = "Influential Observations")

```
```{r}
# Extract coefficients from the updated final model
coefficients_updated <- summary(final_model_updated)$coefficients

# Create a summary table
summary_table_updated <- data.frame(
  Variable = rownames(coefficients_updated),
  Estimate = coefficients_updated[, "Estimate"],
  Std_Error = coefficients_updated[, "Std. Error"],
  z_value = coefficients_updated[, "z value"],
  p_value = coefficients_updated[, "Pr(>|z|)"]
)

# Calculate Odds Ratios and Confidence Intervals
summary_table_updated <- summary_table_updated %>%
  mutate(
    Odds_Ratio = exp(Estimate),
    CI_Lower = exp(Estimate - 1.96 * Std_Error),
    CI_Upper = exp(Estimate + 1.96 * Std_Error)
  ) %>%
  select(Variable, Estimate, Odds_Ratio, CI_Lower, CI_Upper, p_value)

# Display the summary table
kable(summary_table_updated, digits = 3, caption = "Final Logistic Regression Model Summary")

```
















