---
title: "p8130_final_project_2"
output: pdf_document
date: "2024-12-19"
---

# Project 2: Breast cancer survival prediction

```{r, include=FALSE}
library(tidyverse)
library(knitr)
library(car) 
library(e1071)
library(glmnet)
```

## Data exploration

### Descriptive table with summary statistics

```{r}
data <- read.csv("Project_2_data.csv")
head(data,10)

numerical_summary <- data %>%
  select_if(is.numeric) %>%
  summarise_all(list(
    count = ~sum(!is.na(.)),
    mean = mean,
    std = sd,
    min = min,
    median = median,
    max = max
  )) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Variable", "Statistic"), sep = "_")

formatted_summary <- numerical_summary %>%
  pivot_wider(names_from = Statistic, values_from = Value)

kable(formatted_summary, col.names = c("Variable", "Count", "Mean", "Std", "Min", "Median", "Max"), caption = "Numerical Variables Summary Statistics")
```

```{r}
categorical_vars <- data %>% select_if(is.character)

category_summary <- categorical_vars %>%
  gather(Variable, Category) %>%
  group_by(Variable, Category) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = round((Count / sum(Count)) * 100, 2)) %>%
  arrange(Variable, desc(Count))

formatted_summary <- category_summary %>%
  group_by(Variable) %>%
  mutate(Variable = ifelse(row_number() == 1, Variable, ""))

kable(formatted_summary, col.names = c("Variable", "Category", "Count", "Percentage (%)"), caption = "Category Distribution of Categorical Variables")
```

### Explore the Distribution of the Outcome (Status: Dead / Alive)
```{r}
status_distribution <- data %>%
  group_by(Status) %>%
  summarise(Count = n()) %>%
  mutate(Proportion = Count / sum(Count))

kable(status_distribution, col.names = c("Status", "Count", "Proportion"), caption = "Distribution of Survival Status (Dead/Alive)")

ggplot(data, aes(x = Status, fill = Status)) +
  geom_bar() +
  labs(title = "Distribution of Survival Status", x = "Status", y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("lightblue", "pink"))
```

For logistic regression, the binary outcome variable (Status: Dead/Alive) does not require transformation, as logistic regression inherently models binary outcomes.

### Transformation
```{r}
# Identify numerical variables
numerical_vars <- data %>%
  select_if(is.numeric) %>%
  select(-`Survival.Months`)

# Display the list of numerical variables
names(numerical_vars)
# Convert Status to a binary numeric variable
data$Status <- ifelse(data$Status == "Dead", 1, 0)

# Scatterplots for each numerical variable against the logit
logit <- function(p) log(p / (1 - p))  # Logit function

numerical_vars %>%
  names() %>%
  map(~ ggplot(data, aes(x = .data[[.x]], y = Status)) +
        stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
        geom_point(alpha = 0.5) +
        labs(title = paste("Relationship Between", .x, "and Logit of Status"), 
             x = .x, 
             y = "Logit(Status)") +
        theme_minimal())

# Calculate skewness for numerical variables
numerical_skewness <- numerical_vars %>%
  map_df(~ tibble(Variable = deparse(substitute(.)),
                  Skewness = skewness(., na.rm = TRUE)))

# Correct the Variable column
numerical_skewness <- tibble(
  Variable = colnames(numerical_vars),
  Skewness = sapply(numerical_vars, skewness, na.rm = TRUE)
)

# Display the skewness table
kable(numerical_skewness, col.names = c("Variable", "Skewness"), caption = "Skewness of Numerical Variables")
```

After our initial detection, we found out that: 

* Reginol.Node.Positive variable show slightly nonlinear with the logit of Status, it need transformation.

* The skewness analysis reveals that Age (-0.22) has a roughly symmetric distribution, requiring no transformation. Tumor Size (1.74) shows moderate right skewness, suggesting a potential log transformation to normalize the distribution, though it may not be strictly necessary. Regional Node Examined (0.83) has mild positive skewness and can likely be retained in its current form unless further diagnostics indicate otherwise. Reginol Node Positive (2.70), with significant right skewness, would benefit from a log transformation to reduce skewness and stabilize its relationship with the logit in the logistic regression model. These adjustments ensure numerical variables are well-prepared for regression analysis.

Base on the analysis above, try to make log transformation on Reginol Node Positive & Tumor Size.

```{r}
data <- data %>%
  mutate(
    Log_Reginol_Node_Positive = log1p(`Reginol.Node.Positive`),
    Log_Tumor_Size = log1p(`Tumor.Size`)
  )
transformed_skewness <- data %>%
  select(Log_Reginol_Node_Positive, Log_Tumor_Size) %>%
  summarise_all(~ skewness(.))

# Combine with variable names
transformed_skewness_table <- tibble(
  Variable = c("Log_Reginol_Node_Positive", "Log_Tumor_Size"),
  Skewness = as.numeric(transformed_skewness)
)

# Display the updated skewness table
kable(transformed_skewness_table, col.names = c("Variable", "Skewness"), caption = "Skewness of Transformed Variables")

## Plots for Transformed Variables Against Logit of Status
logit <- function(p) log(p / (1 - p))  # Logit function

# Plot for Log_Reginol_Node_Positive
ggplot(data, aes(x = Log_Reginol_Node_Positive, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Reginol_Node_Positive and Logit of Status", x = "Log_Reginol_Node_Positive", y = "Logit(Status)") +
  theme_minimal()

# Plot for Log_Tumor_Size
ggplot(data, aes(x = Log_Tumor_Size, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Tumor_Size and Logit of Status", x = "Log_Tumor_Size", y = "Logit(Status)") +
  theme_minimal()
```

Comments:

* For Log_Reginol_Node_Positive, the skewness improved from 2.70 to 0.99, indicating a significant reduction in skewness. While still slightly positively skewed, the value is now within an acceptable range for modeling.

* For Log_Tumor_Size, the skewness reduced from 1.74 to -0.09, making it almost symmetric. This transformation effectively normalized the variable.

* For Log_Reginol_Node_Positive, the log transformation on Reginol.Node.Positive likely improved its relationship with the logit

* For Log_Tumor_Size, the curvature is still present after the transformation, and the linearity with the logit has not significantly improved. 

As a result, we should definitely conduct a log transformation on Log_Reginol_Node_Positive, we are not sure on Log_Tumor_Size, we can evaluate it in model selection.

```{r}
data=data |>
  select(-`Reginol.Node.Positive`, -`Tumor.Size`)
```

```{r}
# check for multicollinearity among numeric variables
selected_vars <- c("Age", "Log_Reginol_Node_Positive", "Log_Tumor_Size","Regional.Node.Examined")

subset_data <- data[, selected_vars]

if (all(sapply(subset_data, is.numeric))) {
  correlation_matrix <- cor(subset_data, use = "pairwise.complete.obs")
  print(correlation_matrix)
}
```

None of the correlation coefficients between numeric variables exceed 0.5, indicating that there is no strong linear relationship between each pair of numeric variables.

```{r}
# Checking for highly consistent category variables

category1 <- "differentiate"
category2 <- "Grade"

contingency_table <- table(data[[category1]], data[[category2]])
print(contingency_table)
```

We discover that complete linear dependency exist among Grade and differentiate, so we can only include one of them in the prediction model, so differentiate is excluded.

Finally, we need to change all the catagorical variables to dummy variables:
```{r}
categorical_vars <- data %>%
  select_if(is.character) %>%
  names()

data_final <- data %>%
  mutate(across(all_of(categorical_vars), ~ as.factor(.))) %>%  
  model.matrix(~ . - 1, data = .) %>%  
  as.data.frame() 

head(data_final,10)
```

Lasso + model:
```{r}
x <- model.matrix(Status ~ ., data = data_final)[, -1]  # 去掉截距列
y <- data_final$Status

# 使用 Lasso 进行变量筛选
lasso_cv <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# 获取最佳正则化参数 lambda
best_lambda <- lasso_cv$lambda.min
print(paste("Optimal lambda:", best_lambda))

# 使用最佳 lambda 拟合 Lasso 模型
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best_lambda)

# 提取 Lasso 模型的系数
lasso_coefficients <- coef(lasso_model)

# 转换为标准矩阵格式
lasso_coefficients_matrix <- as.matrix(lasso_coefficients)

# 提取非零系数对应的变量名（去掉截距项）
selected_vars <- rownames(lasso_coefficients_matrix)[lasso_coefficients_matrix != 0][-1]

# 输出筛选出的变量
print("Selected variables:")
print(selected_vars)

# 构建最终的逻辑回归模型
final_formula <- as.formula(paste("Status ~", paste(selected_vars, collapse = " + ")))
final_model <- glm(final_formula, data = data_final, family = "binomial")

# 输出最终模型的摘要
summary(final_model)

```
```{r}
# Identify aliased (linearly dependent) coefficients
alias_info <- alias(final_model)
print(alias_info)

# Extract the names of aliased coefficients, if any
if (!is.null(alias_info$Complete)) {
  aliased_vars <- rownames(alias_info$Complete)
  print("Aliased (linearly dependent) variables:")
  print(aliased_vars)
} else {
  print("No aliased coefficients found.")
}

```


```{r}
# Check if there are aliased coefficients
if (!is.null(alias_info$Complete)) {
  # Extract the names of aliased variables
  aliased_vars <- rownames(alias_info$Complete)
  
  # Print aliased variables
  print("Aliased (linearly dependent) variables:")
  print(aliased_vars)
  
  # Strategy: Remove the last variable in each aliased set
  # This can be adjusted based on domain knowledge or other criteria
  vars_to_remove <- sapply(strsplit(aliased_vars, ":"), tail, n = 1)
  
  print("Variables to remove to resolve aliasing:")
  print(vars_to_remove)
  
  # Update the selected_vars by removing the problematic variables
  selected_vars_updated <- setdiff(selected_vars, vars_to_remove)
  
  print("Updated list of selected variables after removing aliased variables:")
  print(selected_vars_updated)
  
  # Reconstruct the final formula without the aliased variables
  final_formula_updated <- as.formula(paste("Status ~", paste(selected_vars_updated, collapse = " + ")))
  
  # Refit the logistic regression model without aliased variables
  final_model_updated <- glm(final_formula_updated, data = data_final, family = "binomial")
  
  
} else {
  print("No aliased coefficients to address. Proceeding to calculate VIF.")
  final_model_updated <- final_model  # No changes needed
}

```
```{r}
# Calculate Variance Inflation Factor (VIF) for the updated model
vif_values_updated <- vif(final_model_updated)
print("Variance Inflation Factors (VIF) for the updated model:")
print(vif_values_updated)

# Optionally, visualize VIF values
library(ggplot2)

# Create a data frame for plotting
vif_df <- data.frame(
  Variable = names(vif_values_updated),
  VIF = vif_values_updated
)

# Plot VIF values
ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_hline(yintercept = 5, color = "red", linetype = "dashed") +
  labs(title = "Variance Inflation Factors (VIF)",
       x = "Predictor Variables",
       y = "VIF") +
  coord_flip() +
  theme_minimal()


```
```{r}

threshold <- 0.5 
# Predict probabilities using the updated model
pred_probs_updated <- predict(final_model_updated, type = "response")

# Set the same classification threshold
pred_classes_updated <- ifelse(pred_probs_updated >= threshold, 1, 0)

# Generate the confusion matrix for the updated model
conf_matrix_updated <- confusionMatrix(as.factor(pred_classes_updated), as.factor(y), positive = "1")
print(conf_matrix_updated)

```
```{r}
# Install and load necessary package
if(!require(pROC)) install.packages("pROC")
library(pROC)

# Compute ROC curve
roc_obj <- roc(y, pred_probs_updated)

# Plot ROC curve
plot(roc_obj, col = "blue", main = "ROC Curve for Final Logistic Regression Model")
abline(a = 0, b = 1, lty = 2, col = "red")  # Diagonal line for reference

# Calculate and print AUC
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 3)))

```
```{r}
# Install and load necessary package
if(!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Create calibration data by binning predicted probabilities
calibration_data <- data.frame(
  predicted = pred_probs_updated,
  actual = y
)

# Plot calibration
ggplot(calibration_data, aes(x = predicted, y = actual)) +
  stat_summary_bin(fun = mean, bins = 10, geom = "line", color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Calibration Plot", x = "Predicted Probability", y = "Observed Proportion") +
  theme_minimal()

```

```{r}
# Ensure 'Race' is included in data_final
if(!"Race" %in% colnames(data_final)){
  data_final$Race <- data$Race
}

# Define race groups: "White" and "Black_Other"
data_final <- data_final %>%
  mutate(Race_Group = ifelse(Race == "White", "White", "Black_Other"))

# Function to compute performance metrics
compute_metrics <- function(actual, predicted, probs, group_name) {
  cm <- confusionMatrix(as.factor(predicted), as.factor(actual), positive = "1")
  roc_obj <- roc(actual, probs)
  auc_val <- auc(roc_obj)
  
  return(data.frame(
    Group = group_name,
    Accuracy = round(cm$overall['Accuracy'], 3),
    Precision = round(cm$byClass['Precision'], 3),
    Recall = round(cm$byClass['Recall'], 3),
    F1 = round(cm$byClass['F1'], 3),
    AUC = round(auc_val, 3)
  ))
}

# Initialize list to store metrics
metrics_list <- list()

# Iterate over each race group
for(group in unique(data_final$Race_Group)) {
  idx <- which(data_final$Race_Group == group)
  actual_group <- y[idx]
  probs_group <- pred_probs_updated[idx]
  predicted_group <- pred_classes_updated[idx]
  
  metrics <- compute_metrics(actual_group, predicted_group, probs_group, group)
  metrics_list[[group]] <- metrics
}

# Combine metrics into a single data frame
metrics_df <- bind_rows(metrics_list)
print(metrics_df)


```
```{r}
# Compute ROC curves for each group
roc_white <- roc(y[data_final$Race_Group == "White"], pred_probs_updated[data_final$Race_Group == "White"])
roc_black_other <- roc(y[data_final$Race_Group == "Black_Other"], pred_probs_updated[data_final$Race_Group == "Black_Other"])

# Perform DeLong's test to compare AUCs
delong_test <- roc.test(roc_white, roc_black_other, method = "delong")
print(delong_test)

```


```{r}
# Calculate Cook's Distance
cooks_d <- cooks.distance(final_model_updated)

# Plot Cook's Distance
plot(cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4/(nrow(data_final) - length(final_model_updated$coefficients) - 1), col = "red", lty = 2)

# Identify influential points
influential_threshold <- 4/(nrow(data_final) - length(final_model_updated$coefficients) - 1)
influential_points <- which(cooks_d > influential_threshold)
print(paste("Number of influential points:", length(influential_points)))

# Inspect influential observations
influential_data <- data_final[influential_points, ]
kable(head(influential_data, 10), caption = "Influential Observations")

```
```{r}
# Extract coefficients from the updated final model
coefficients_updated <- summary(final_model_updated)$coefficients

# Create a summary table
summary_table_updated <- data.frame(
  Variable = rownames(coefficients_updated),
  Estimate = coefficients_updated[, "Estimate"],
  Std_Error = coefficients_updated[, "Std. Error"],
  z_value = coefficients_updated[, "z value"],
  p_value = coefficients_updated[, "Pr(>|z|)"]
)

# Calculate Odds Ratios and Confidence Intervals
summary_table_updated <- summary_table_updated %>%
  mutate(
    Odds_Ratio = exp(Estimate),
    CI_Lower = exp(Estimate - 1.96 * Std_Error),
    CI_Upper = exp(Estimate + 1.96 * Std_Error)
  ) %>%
  select(Variable, Estimate, Odds_Ratio, CI_Lower, CI_Upper, p_value)

# Display the summary table
kable(summary_table_updated, digits = 3, caption = "Final Logistic Regression Model Summary")

```
















