---
title: "p8130_final_project_2"
output:
  word_document: default
  pdf_document: default
date: "2024-12-19"
---

# Project 2: Breast cancer survival prediction

```{r, include=FALSE}
library(tidyverse)
library(knitr)
library(car) 
library(e1071)
library(glmnet)
library(caret)
library(pROC)
```

## Data exploration

### Descriptive table with summary statistics

```{r}
data <- read.csv("Project_2_data.csv")
head(data,10)

numerical_summary <- data %>%
  select_if(is.numeric) %>%
  summarise_all(list(
    count = ~sum(!is.na(.)),
    mean = mean,
    std = sd,
    min = min,
    median = median,
    max = max
  )) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Variable", "Statistic"), sep = "_")

formatted_summary <- numerical_summary %>%
  pivot_wider(names_from = Statistic, values_from = Value)

kable(formatted_summary, col.names = c("Variable", "Count", "Mean", "Std", "Min", "Median", "Max"), caption = "Numerical Variables Summary Statistics")
```

```{r}
categorical_vars <- data %>% select_if(is.character)

category_summary <- categorical_vars %>%
  gather(Variable, Category) %>%
  group_by(Variable, Category) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = round((Count / sum(Count)) * 100, 2)) %>%
  arrange(Variable, desc(Count))

formatted_summary <- category_summary %>%
  group_by(Variable) %>%
  mutate(Variable = ifelse(row_number() == 1, Variable, ""))

kable(formatted_summary, col.names = c("Variable", "Category", "Count", "Percentage (%)"), caption = "Category Distribution of Categorical Variables")
```

### Explore the Distribution of the Outcome (Status: Dead / Alive)
```{r}
status_distribution <- data %>%
  group_by(Status) %>%
  summarise(Count = n()) %>%
  mutate(Proportion = Count / sum(Count))

kable(status_distribution, col.names = c("Status", "Count", "Proportion"), caption = "Distribution of Survival Status (Dead/Alive)")

ggplot(data, aes(x = Status, fill = Status)) +
  geom_bar() +
  labs(title = "Distribution of Survival Status", x = "Status", y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("lightblue", "pink"))
```

For logistic regression, the binary outcome variable (Status: Dead/Alive) does not require transformation, as logistic regression inherently models binary outcomes.

### Transformation
```{r}
# Identify numerical variables
numerical_vars <- data %>%
  select_if(is.numeric) %>%
  select(-`Survival.Months`)

# Display the list of numerical variables
names(numerical_vars)
# Convert Status to a binary numeric variable
data$Status <- ifelse(data$Status == "Dead", 1, 0)

# Scatterplots for each numerical variable against the logit
logit <- function(p) log(p / (1 - p))  # Logit function

numerical_vars %>%
  names() %>%
  map(~ ggplot(data, aes(x = .data[[.x]], y = Status)) +
        stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
        geom_point(alpha = 0.5) +
        labs(title = paste("Relationship Between", .x, "and Logit of Status"), 
             x = .x, 
             y = "Logit(Status)") +
        theme_minimal())

# Calculate skewness for numerical variables
numerical_skewness <- numerical_vars %>%
  map_df(~ tibble(Variable = deparse(substitute(.)),
                  Skewness = skewness(., na.rm = TRUE)))

# Correct the Variable column
numerical_skewness <- tibble(
  Variable = colnames(numerical_vars),
  Skewness = sapply(numerical_vars, skewness, na.rm = TRUE)
)

# Display the skewness table
kable(numerical_skewness, col.names = c("Variable", "Skewness"), caption = "Skewness of Numerical Variables")
```

After our initial detection, we found out that: 

* Reginol.Node.Positive variable show slightly nonlinear with the logit of Status, it need transformation.

* The skewness analysis reveals that Age (-0.22) has a roughly symmetric distribution, requiring no transformation. Tumor Size (1.74) shows moderate right skewness, suggesting a potential log transformation to normalize the distribution, though it may not be strictly necessary. Regional Node Examined (0.83) has mild positive skewness and can likely be retained in its current form unless further diagnostics indicate otherwise. Reginol Node Positive (2.70), with significant right skewness, would benefit from a log transformation to reduce skewness and stabilize its relationship with the logit in the logistic regression model. These adjustments ensure numerical variables are well-prepared for regression analysis.

Base on the analysis above, try to make log transformation on Reginol Node Positive & Tumor Size.

```{r}
data <- data %>%
  mutate(
    Log_Reginol_Node_Positive = log1p(`Reginol.Node.Positive`),
    Log_Tumor_Size = log1p(`Tumor.Size`)
  )
transformed_skewness <- data %>%
  select(Log_Reginol_Node_Positive, Log_Tumor_Size) %>%
  summarise_all(~ skewness(.))

# Combine with variable names
transformed_skewness_table <- tibble(
  Variable = c("Log_Reginol_Node_Positive", "Log_Tumor_Size"),
  Skewness = as.numeric(transformed_skewness)
)

# Display the updated skewness table
kable(transformed_skewness_table, col.names = c("Variable", "Skewness"), caption = "Skewness of Transformed Variables")

## Plots for Transformed Variables Against Logit of Status
logit <- function(p) log(p / (1 - p))  # Logit function

# Plot for Log_Reginol_Node_Positive
ggplot(data, aes(x = Log_Reginol_Node_Positive, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Reginol_Node_Positive and Logit of Status", x = "Log_Reginol_Node_Positive", y = "Logit(Status)") +
  theme_minimal()

# Plot for Log_Tumor_Size
ggplot(data, aes(x = Log_Tumor_Size, y = Status)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship Between Log_Tumor_Size and Logit of Status", x = "Log_Tumor_Size", y = "Logit(Status)") +
  theme_minimal()
```

Comments:

* For Log_Reginol_Node_Positive, the skewness improved from 2.70 to 0.99, indicating a significant reduction in skewness. While still slightly positively skewed, the value is now within an acceptable range for modeling.

* For Log_Tumor_Size, the skewness reduced from 1.74 to -0.09, making it almost symmetric. This transformation effectively normalized the variable.

* For Log_Reginol_Node_Positive, the log transformation on Reginol.Node.Positive likely improved its relationship with the logit as well. 

* For Log_Tumor_Size, after the transformation the linearity with the logit has not significantly improved, but at least the skewness reduced.

As a result, we should definitely conduct a log transformation on Reginol.Node.Positive and Tumor.Size (so the two original variables can be removed).

```{r}
data=data |>
  select(-`Reginol.Node.Positive`, -`Tumor.Size`)
```

```{r}
# Compute correlation coefficients among numeric variables
selected_vars <- c("Age", "Log_Reginol_Node_Positive", "Log_Tumor_Size","Regional.Node.Examined")

subset_data <- data[, selected_vars]

if (all(sapply(subset_data, is.numeric))) {
  correlation_matrix <- cor(subset_data, use = "pairwise.complete.obs")
  print(correlation_matrix)
}
```

None of the correlation coefficients between numeric variables exceed 0.5, indicating that there is no strong linear relationship between each pair of numeric variables.

```{r}
# Identify highly consistent category variables
contingency_table <- table(data[["differentiate"]], data[["Grade"]])
print(contingency_table)
```

We discover that complete linear dependency exist among Grade and differentiate, so we can only include one of them in the prediction model, so differentiate is excluded.

```{r}
data=data |>
  select(-differentiate)
```

Finally, we need to change all the catagorical variables to dummy variables:
```{r}
categorical_vars <- data %>%
  select_if(is.character) %>%
  names()

data_final <- data %>%
  mutate(across(all_of(categorical_vars), ~ as.factor(.))) %>%  
  model.matrix(~ . - 1, data = .) %>%  
  as.data.frame() 

data_final = data_final |>
  select(-`Survival.Months`)

head(data_final,10)
```

After basic data preprocessing, we use lasso regression to help select the variables used as predictors.

```{r}
x <- model.matrix(Status ~ ., data = data_final)[, -1]
y <- data_final$Status

# Perform cross-validation for Lasso regression
lasso_cv <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# Get the optimal regularization parameter lambda
best_lambda <- lasso_cv$lambda.min
print(paste("Optimal lambda:", best_lambda))

# Fit the Lasso model using the optimal lambda
lasso_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = best_lambda)

# Extract the coefficients from the Lasso model
lasso_coefficients <- coef(lasso_model)

# Convert coefficients to a standard matrix format
lasso_coefficients_matrix <- as.matrix(lasso_coefficients)

# Extract variable names with non-zero coefficients (excluding the intercept)
selected_vars <- rownames(lasso_coefficients_matrix)[lasso_coefficients_matrix[, 1] != 0][-1]

# Output the selected variables
print("Selected variables:")
print(selected_vars)

# Construct the logistic regression formula
final_formula <- as.formula(paste("Status ~", paste(selected_vars, collapse = " + ")))

# Fit the final logistic regression model
final_model <- glm(final_formula, data = data_final, family = "binomial")

# Output the summary of the final model
summary(final_model)
```

```{r}
# Check for linear dependencies (aliased variables)
alias_info <- alias(final_model)
print(alias_info)

if (!is.null(alias_info$Complete)) {
  aliased_vars <- rownames(alias_info$Complete)
  print("Aliased (linearly dependent) variables:")
  print(aliased_vars)
} else {
  print("No aliased coefficients found.")
}
```

X6th.StageIIIC was identified as an aliased (linearly dependent) variable, being perfectly correlated with N.StageN3. This redundancy can cause multicollinearity issues and instability in coefficient estimation. Both X6th.StageIIIC and N.StageN3 were removed to ensure a more stable and interpretable model.

```{r}
# Remove X6th.StageIIIC and N.StageN3 from selected_vars
vars_to_remove <- c("X6th.StageIIIC", "N.StageN3")
selected_vars_updated <- setdiff(selected_vars, vars_to_remove)

# Output the updated selected variables
print("Updated selected variables:")
print(selected_vars_updated)

# Construct the updated logistic regression formula
updated_formula <- as.formula(paste("Status ~", paste(selected_vars_updated, collapse = " + ")))

# Fit the updated logistic regression model
final_model_updated <- glm(updated_formula, data = data_final, family = "binomial")

# Output the summary of the updated model
summary(final_model_updated)
```

```{r}
# Calculate Variance Inflation Factor (VIF) for the updated model
vif_values_updated <- vif(final_model_updated)
print("Variance Inflation Factors (VIF) for the updated model:")
print(vif_values_updated)

vif_df <- data.frame(
  Variable = names(vif_values_updated),
  VIF = vif_values_updated
)

ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_hline(yintercept = 5, color = "red", linetype = "dashed") +
  labs(title = "Variance Inflation Factors (VIF)",
       x = "Predictor Variables",
       y = "VIF") +
  coord_flip() +
  theme_minimal()
```

After removing X6th.StageIIIC and N.StageN3 and refitting the updated logistic regression model, the Variance Inflation Factor (VIF) values were recalculated. The results indicate that all predictor variables now have VIF values below 5, suggesting that multicollinearity among the independent variables has been successfully resolved.

We can extract coefficients from the updated final model and show them.

```{r}
# Extract coefficients from the updated final model
coefficients_updated <- summary(final_model_updated)$coefficients

# Create a summary table
summary_table_updated <- data.frame(
  Variable = rownames(coefficients_updated),
  Estimate = coefficients_updated[, "Estimate"],
  Std_Error = coefficients_updated[, "Std. Error"],
  z_value = coefficients_updated[, "z value"],
  p_value = coefficients_updated[, "Pr(>|z|)"]
)

# Calculate Odds Ratios and Confidence Intervals
summary_table_updated <- summary_table_updated %>%
  mutate(
    Odds_Ratio = exp(Estimate),
    CI_Lower = exp(Estimate - 1.96 * Std_Error),
    CI_Upper = exp(Estimate + 1.96 * Std_Error)
  ) %>%
  select(Variable, Estimate, Odds_Ratio, CI_Lower, CI_Upper, p_value)

# Display the summary table
kable(summary_table_updated, digits = 3, caption = "Final Logistic Regression Model Summary")
```

Then we need to evaluate model performance by computing some matrix. Set the threshold to 0.5, meaning that any predicted probability greater than or equal to 0.5 is classified as the positive class (1) (in our case, dead), while probabilities below 0.5 are classified as the negative class (0) (in our case, alive). 

```{r}
threshold <- 0.5 
# Predict probabilities using the updated model
pred_probs_updated <- predict(final_model_updated, type = "response")

pred_classes_updated <- ifelse(pred_probs_updated >= threshold, 1, 0)

# Generate the confusion matrix for the updated model
conf_matrix_updated <- confusionMatrix(as.factor(pred_classes_updated), as.factor(y), positive = "1")
print(conf_matrix_updated)
```

We also need to draw the Receiver Operating Characteristic Curve.

```{r}
# Predict probabilities for the entire dataset
pred_probs_updated <- predict(final_model_updated, type = "response")

# Compute ROC curve
roc_curve <- roc(data_final$Status, pred_probs_updated)

# Plot ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve for Updated Model")
abline(a = 0, b = 1, lty = 2, col = "gray")  # Add diagonal line (random guess)

# Add AUC value to the plot
auc_value <- auc(roc_curve)
text(0.6, 0.2, paste("AUC =", round(auc_value, 3)), col = "red", cex = 1.2)

```

On the entire dataset, ROC-AUC value is 0.753, indicating the model performance is acceptable but has room for improvement (0.7 - 0.8).

```{r}
# Create calibration data frame
calibration_df <- data.frame(
  Predicted = pred_probs_updated,
  Observed = as.numeric(data_final$Status)
)

# Group predicted probabilities into bins
calibration_df$Bin <- cut(calibration_df$Predicted, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)

# Calculate mean predicted probability and observed proportion for each bin
calibration_summary <- calibration_df %>%
  group_by(Bin) %>%
  summarise(
    Mean_Predicted = mean(Predicted),
    Mean_Observed = mean(Observed)
  )

# Plot calibration curve
ggplot(calibration_summary, aes(x = Mean_Predicted, y = Mean_Observed)) +
  geom_point(color = "blue", size = 3) +
  geom_line(color = "blue", lwd = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Calibration Curve for Updated Model",
       x = "Mean Predicted Probability",
       y = "Observed Proportion") +
  theme_minimal()

```

The calibration curve demonstrates that the predicted probabilities align reasonably well with the observed proportions across most bins, as the points and blue line generally follow the diagonal red dashed line (perfect calibration) although some deviations exist. Overall, the model shows acceptable calibration.

Cross-validation provides a comprehensive method for model diagnosis by evaluating its performance across multiple data splits. This approach helps assess the model's generalization ability, reducing the risk of overfitting and ensuring robust performance on unseen data.

```{r}
# Define 10-fold cross-validation
set.seed(123)  # For reproducibility
folds <- createFolds(y, k = 10, list = TRUE)

# Initialize a data frame to store results
cv_results <- data.frame(
  Fold = integer(),
  Accuracy = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  ROC_AUC = numeric()
)

# Perform 10-fold cross-validation
for (i in seq_along(folds)) {
  # Split data into training and testing sets
  train_indices <- unlist(folds[-i])  # Indices for training data
  test_indices <- unlist(folds[i])   # Indices for testing data
  
  train_data <- data_final[train_indices, ]
  test_data <- data_final[test_indices, ]
  
  # Refit the logistic regression model on the training set
  train_model <- glm(updated_formula, data = train_data, family = "binomial")
  
  # Predict probabilities on the testing set
  test_probs <- predict(train_model, newdata = test_data, type = "response")
  
  # Convert probabilities to binary predictions
  test_preds <- ifelse(test_probs >= threshold, 1, 0)
  
  # Generate the confusion matrix for the testing set
  fold_conf_matrix <- confusionMatrix(
    as.factor(test_preds),
    as.factor(test_data$Status),
    positive = "1"
  )
  
  # Calculate ROC-AUC
  roc_curve <- roc(as.numeric(test_data$Status), test_probs)
  roc_auc <- auc(roc_curve)
  
  # Store performance metrics for this fold
  cv_results <- rbind(cv_results, data.frame(
    Fold = i,
    Accuracy = fold_conf_matrix$overall["Accuracy"],
    Sensitivity = fold_conf_matrix$byClass["Sensitivity"],
    Specificity = fold_conf_matrix$byClass["Specificity"],
    ROC_AUC = as.numeric(roc_auc)
  ))
}

# Summarize cross-validation results
cv_summary <- data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity", "ROC_AUC"),
  Mean = colMeans(cv_results[, -1], na.rm = TRUE),
  SD = apply(cv_results[, -1], 2, sd, na.rm = TRUE)
)

# Print the summary of cross-validation results
print("Cross-Validation Results:")
print(cv_summary)
```

The average accuracy is 85.29% (SD = 1.04%), indicating that the model performs well overall in classifying the observations correctly. The specificity is very high, averaging 98.36% (SD = 0.64%), which demonstrates the model's strong ability to correctly identify negative cases (Alive). However, the sensitivity is relatively low at 13.00% (SD = 3.01%), reflecting a limited capability to detect positive cases (Alive). The ROC-AUC is 0.744 (SD = 0.024), suggesting the model has acceptable discrimination ability but room for improvement.

```{r}
# Calculate Cook's Distance
cooks_d <- cooks.distance(final_model_updated)

# Plot Cook's Distance
plot(cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4/(nrow(data_final) - length(final_model_updated$coefficients) - 1), col = "red", lty = 2)



```
The red dashed line represents the commonly used threshold for identifying influential points. The majority of observations fall below the threshold, suggesting that they contribute reasonably and do not overly influence the model.

